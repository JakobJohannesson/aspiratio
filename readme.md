## The Stock Archive

### Project Overview

An automated system to download and validate annual reports for Swedish OMXS30 companies (2019-2024). The pipeline discovers investor relations URLs, downloads reports, and validates them for completeness.

**Status**: âœ… Core pipeline complete with validation feedback loop

### Workflow

**IMPORTANT: Always check what's missing before downloading!**

#### Using CLI Commands (Recommended)

After installation with `pip install -e .`, use the clean CLI commands:

```bash
# Step 1: Check what's missing from coverage table
python3 -c "
import pandas as pd
df = pd.read_csv('coverage_table_updated.csv', sep='\t')
missing = df[df['Priority'] != 'Complete âœ“']
print(f'Missing: {len(missing)} reports')
by_company = missing.groupby('CompanyName').size().sort_values(ascending=False)
for company, count in by_company.items():
    print(f'  {company}: {count}')
"

# Step 2: Download reports (only fetches missing ones)
aspiratio-download

# Step 3: Validate downloaded PDFs
aspiratio-validate

# Step 4: Update coverage table
aspiratio-update

# Step 5: Retry failures with smart logic (Playwright fallback)
aspiratio-retry

# Interactive UI
streamlit run scripts/app.py
```

#### Using Python Scripts (Alternative)

```bash
# Traditional method still works
python scripts/download_reports.py
python scripts/validate_reports.py
python scripts/update_coverage_table.py
python scripts/redownload_failed.py
```

**Key principles:**
- **Check first**: Always verify what's missing in coverage table before downloading
- **Download only missing**: Script automatically filters to Priority != "Complete âœ“"
- **Validate immediately**: Each PDF is validated right after download
- **Update coverage**: Coverage table is updated after validation with status
- **Try alternatives**: If multiple PDFs found for a year, tries each until one validates

### Key Features

- **Smart Filtering**: Excludes quarterly/interim reports and SEC filings (Form 20-F, Form SD)
- **Validation Pipeline**: Verifies page count (50-500), company name, and year in PDF content
- **Priority System**: Automatic prioritization based on validation results
- **Pattern Learning**: Direct URL patterns for JavaScript-heavy sites (ASSA ABLOY)
- **Playwright Support**: Handles dynamic content for complex sites (ABB Group Annual Reports)
- **Encrypted PDFs**: Supports password-protected documents
- **Progress Tracking**: Coverage table with milestone tracking and priorities
- **Enhanced Error Handling**: Detailed diagnostics for connection issues (DNS, timeout, HTTP errors)
- **User Agent Rotation**: Automatic rotation through multiple browser user agents to avoid blocking

### Troubleshooting

**Connection Issues:**

The system now provides detailed error messages for different types of connection failures:

- ğŸŒ **DNS Resolution Error**: Domain cannot be resolved (may be blocked or unreachable in current environment)
- â± **Connection Timeout**: Request took too long (increase timeout in config.yaml)
- ğŸš« **HTTP 403 Forbidden**: Server is blocking requests (may need Playwright approach)
- âŒ **HTTP 404 Not Found**: URL does not exist (may need to update IR URL)
- ğŸ”’ **SSL/TLS Error**: Certificate validation issue

To diagnose connection issues:
```bash
# Test all company URLs
aspiratio-diagnose

# Test a specific URL
aspiratio-diagnose https://www.example.com/investors
```

The diagnostic tool will:
1. Test the URL with multiple user agents
2. Identify the specific error type (DNS, timeout, HTTP error, etc.)
3. Provide recommendations for resolution
4. Save detailed results to `connection_diagnostics.csv`

**Configuration Options:**

Adjust timeouts and retry behavior in `config.yaml`:
```yaml
download:
  max_retries: 3              # Attempts per download with user agent rotation
  request_timeout: 30         # Seconds to wait for response
  max_consecutive_failures: 3 # Max failures before giving up
```

### Current Challenges

- **JavaScript-Heavy Sites**: Some companies (ABB) require Playwright for dynamic content
- **Company-Specific Patterns**: Each company may have unique document structures
- **Year Detection**: Some reports don't consistently mention the year in early pages

### Development

**Setup:**
```bash
# Clone and navigate to project
cd aspiratio

# Install in editable mode (includes all dependencies)
pip install -e .

# Install Playwright browser
playwright install webkit

# Configuration is in config.yaml
# See CONFIG_GUIDE.md for details on using the config system
```

**Available CLI Commands:**
```bash
# Main workflow
aspiratio-download      # Batch download missing reports
aspiratio-validate      # Validate downloaded PDFs
aspiratio-retry         # Smart retry with Playwright fallback
aspiratio-update        # Update coverage table

# Diagnostic tools
aspiratio-diagnose      # Test website connections and identify issues
                       # Usage: aspiratio-diagnose [URL]
                       # Without URL: tests all companies in instrument_master.csv

# Setup tools (one-time)
aspiratio-build-master  # Build/validate instrument master
aspiratio-find-ir       # Find IR URLs for companies

# Interactive
streamlit run scripts/app.py  # Web UI for manual review
```

**Configuration:**
The project uses centralized configuration in `config.yaml`:
- Validation thresholds (min/max pages, confidence)
- Download parameters (retries, timeouts, rate limiting)
- HTTP settings (user agents)
- File paths
- Playwright settings

See [CONFIG_GUIDE.md](CONFIG_GUIDE.md) for usage examples.

**Testing individual companies:**
```python
from aspiratio.utils.report_downloader import download_annual_reports
from aspiratio.config import get_target_years

# Download reports for a specific company
download_annual_reports(
    ir_url="https://www.company.com/investors",
    company_name="Company Name",
    company_id="S1",
    years=get_target_years()  # From config
)
```

### Project Structure

```
aspiratio/
â”œâ”€â”€ config.yaml                  # ğŸ†• Centralized configuration
â”œâ”€â”€ requirements.txt             # ğŸ†• Python dependencies
â”œâ”€â”€ CONFIG_GUIDE.md              # ğŸ†• Configuration usage guide
â”œâ”€â”€ coverage_table_updated.csv   # Download tracking with priorities
â”œâ”€â”€ validation_results.csv       # PDF validation feedback
â”œâ”€â”€ instrument_master.csv        # Company database
â”œâ”€â”€ omxs30_members.csv          # OMXS30 companies
â”œâ”€â”€ aspiratio/
â”‚   â”œâ”€â”€ config.py               # ğŸ†• Configuration loader
â”‚   â””â”€â”€ utils/              
â”‚       â”œâ”€â”€ report_downloader.py     # Tier 1: Traditional scraping
â”‚       â”œâ”€â”€ playwright_downloader.py # Tier 2: JavaScript sites
â”‚       â”œâ”€â”€ ir_search.py             # IR URL discovery
â”‚       â”œâ”€â”€ name_match.py            # Company name matching
â”‚       â””â”€â”€ io.py                    # File operations
â”œâ”€â”€ scripts/                      
â”‚   â”œâ”€â”€ download_reports.py      # Main batch downloader
â”‚   â”œâ”€â”€ validate_reports.py      # PDF validation pipeline
â”‚   â”œâ”€â”€ redownload_failed.py     # Smart retry with Playwright fallback
â”‚   â”œâ”€â”€ app.py                   # Streamlit UI
â”‚   â”œâ”€â”€ record_download.py       # Tier 3: Record manual downloads
â”‚   â”œâ”€â”€ ir_scraper.py            # Find IR URLs
â”‚   â”œâ”€â”€ build_master.py          # Build instrument master
â”‚   â””â”€â”€ update_coverage_table.py # Update coverage tracking
```

### Three-Tier Download Strategy

**Tier 1: Traditional Scraping** (Primary - 80%+ success)
- Searches IR pages for PDF links (HTML, JSON-LD, navigation)
- Failsafe: Main site â†’ IR discovery if initial search fails
- Handles direct patterns for predictable sites

**Tier 2: Playwright/JavaScript** (Fallback - 10-15%)
- For sites with dynamic content (cookie popups, dropdowns)
- Registry-based: Record once, reuse for all years
- Currently implemented: Atlas Copco (S6)

**Tier 3: Manual Recording** (Last Resort - <5%)
- Streamlit app generates Playwright codegen commands
- User demonstrates download path once
- Agent analyzes and integrates pattern

The system automatically escalates: Tier 1 â†’ Tier 2 â†’ Tier 3
â”‚   â”œâ”€â”€ ir_search.py             # IR URL discovery
â”‚   â”œâ”€â”€ name_match.py            # Company name matching
â”‚   â””â”€â”€ io.py                    # File operations
â”œâ”€â”€ scripts/                      
â”‚   â”œâ”€â”€ download_reports.py      # Main download script
â”‚   â”œâ”€â”€ validate_reports.py      # PDF validation pipeline
â”‚   â”œâ”€â”€ redownload_failed.py     # Re-download with better patterns
â”‚   â”œâ”€â”€ app.py                   # Streamlit UI for manual review
â”‚   â”œâ”€â”€ ir_scraper.py            # Find IR URLs
â”‚   â”œâ”€â”€ build_master.py          # Build instrument master
â”‚   â””â”€â”€ update_coverage_table.py # Update coverage tracking
â”œâ”€â”€ companies/{CID}/              # Downloaded PDFs
â””â”€â”€ companies_validated/{CID}/    # Validated PDFs only
```



